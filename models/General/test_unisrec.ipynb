{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PWLayer(nn.Module):\n",
    "    \"\"\"Single Parametric Whitening Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, dropout=0.0):\n",
    "        super(PWLayer, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bias = nn.Parameter(torch.zeros(input_size), requires_grad=True)\n",
    "        self.lin = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(self.dropout(x) - self.bias)\n",
    "\n",
    "\n",
    "class MoEAdaptorLayer(nn.Module):\n",
    "    \"\"\"MoE-enhanced Adaptor\n",
    "    \"\"\"\n",
    "    def __init__(self, n_exps, layers, dropout=0.0, noise=True):\n",
    "        super(MoEAdaptorLayer, self).__init__()\n",
    "\n",
    "        self.n_exps = n_exps\n",
    "        self.noisy_gating = noise\n",
    "\n",
    "        self.experts = nn.ModuleList([PWLayer(layers[0], layers[1], dropout) for i in range(n_exps)])\n",
    "        self.w_gate = nn.Parameter(torch.zeros(layers[0], n_exps), requires_grad=True)\n",
    "        self.w_noise = nn.Parameter(torch.zeros(layers[0], n_exps), requires_grad=True)\n",
    "\n",
    "    def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n",
    "        clean_logits = x @ self.w_gate\n",
    "        if self.noisy_gating and train:\n",
    "            raw_noise_stddev = x @ self.w_noise\n",
    "            noise_stddev = ((F.softplus(raw_noise_stddev) + noise_epsilon))\n",
    "            noisy_logits = clean_logits + (torch.randn_like(clean_logits).to(x.device) * noise_stddev)\n",
    "            logits = noisy_logits\n",
    "        else:\n",
    "            logits = clean_logits\n",
    "\n",
    "        gates = F.softmax(logits, dim=-1)\n",
    "        return gates\n",
    "\n",
    "    def forward(self, x):\n",
    "        gates = self.noisy_top_k_gating(x, self.training) # (B, n_E)\n",
    "        expert_outputs = [self.experts[i](x).unsqueeze(-2) for i in range(self.n_exps)] # [(B, 1, D)]\n",
    "        expert_outputs = torch.cat(expert_outputs, dim=-2)\n",
    "        multiple_outputs = gates.unsqueeze(-1) * expert_outputs\n",
    "        return multiple_outputs.sum(dim=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_adaptor = MoEAdaptorLayer(4, [10, 5], dropout=0.1, noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoEAdaptorLayer(\n",
       "  (experts): ModuleList(\n",
       "    (0): PWLayer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (lin): Linear(in_features=10, out_features=5, bias=False)\n",
       "    )\n",
       "    (1): PWLayer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (lin): Linear(in_features=10, out_features=5, bias=False)\n",
       "    )\n",
       "    (2): PWLayer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (lin): Linear(in_features=10, out_features=5, bias=False)\n",
       "    )\n",
       "    (3): PWLayer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (lin): Linear(in_features=10, out_features=5, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe_adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 10) # (B, D) 直接是把所有embedding给重新变形一下，可以就看作一个封装好的linear层\n",
    "\n",
    "\n",
    "moe_adaptor(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Self-attention layers, a attention score dropout layer is introduced.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the multi-head self-attention layer\n",
    "        attention_mask (torch.Tensor): the attention mask for input tensor\n",
    "\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the multi-head self-attention layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads,\n",
    "        hidden_size,\n",
    "        hidden_dropout_prob,\n",
    "        attn_dropout_prob,\n",
    "        layer_norm_eps,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if hidden_size % n_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, n_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = n_heads\n",
    "        self.attention_head_size = int(hidden_size / n_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.sqrt_attention_head_size = math.sqrt(self.attention_head_size)\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout_prob)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer).permute(0, 2, 1, 3)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer).permute(0, 2, 3, 1)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer)\n",
    "\n",
    "        attention_scores = attention_scores / self.sqrt_attention_head_size\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        # [batch_size heads seq_len seq_len] scores\n",
    "        # [batch_size 1 1 seq_len]\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Point-wise feed-forward layer is implemented by two dense layers.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the point-wise feed-forward layer\n",
    "\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the point-wise feed-forward layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_size, inner_size, hidden_dropout_prob, hidden_act, layer_norm_eps\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense_1 = nn.Linear(hidden_size, inner_size)\n",
    "        self.intermediate_act_fn = self.get_hidden_act(hidden_act)\n",
    "\n",
    "        self.dense_2 = nn.Linear(inner_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def get_hidden_act(self, act):\n",
    "        ACT2FN = {\n",
    "            \"gelu\": self.gelu,\n",
    "            \"relu\": F.relu,\n",
    "            \"swish\": self.swish,\n",
    "            \"tanh\": torch.tanh,\n",
    "            \"sigmoid\": torch.sigmoid,\n",
    "        }\n",
    "        return ACT2FN[act]\n",
    "\n",
    "    def gelu(self, x):\n",
    "        \"\"\"Implementation of the gelu activation function.\n",
    "\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results)::\n",
    "\n",
    "            0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "        \"\"\"\n",
    "        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "    def swish(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        hidden_states = self.dense_1(input_tensor)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        hidden_states = self.dense_2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One transformer layer consists of a multi-head self-attention layer and a point-wise feed-forward layer.\n",
    "\n",
    "    Args:\n",
    "        hidden_states (torch.Tensor): the input of the multi-head self-attention sublayer\n",
    "        attention_mask (torch.Tensor): the attention mask for the multi-head self-attention sublayer\n",
    "\n",
    "    Returns:\n",
    "        feedforward_output (torch.Tensor): The output of the point-wise feed-forward sublayer,\n",
    "                                           is the output of the transformer layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads,\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        hidden_dropout_prob,\n",
    "        attn_dropout_prob,\n",
    "        hidden_act,\n",
    "        layer_norm_eps,\n",
    "    ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            hidden_size,\n",
    "            intermediate_size,\n",
    "            hidden_dropout_prob,\n",
    "            hidden_act,\n",
    "            layer_norm_eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.multi_head_attention(hidden_states, attention_mask)\n",
    "        feedforward_output = self.feed_forward(attention_output)\n",
    "        return feedforward_output\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    r\"\"\"One TransformerEncoder consists of several TransformerLayers.\n",
    "\n",
    "    Args:\n",
    "        n_layers(num): num of transformer layers in transformer encoder. Default: 2\n",
    "        n_heads(num): num of attention heads for multi-head attention layer. Default: 2\n",
    "        hidden_size(num): the input and output hidden size. Default: 64\n",
    "        inner_size(num): the dimensionality in feed-forward layer. Default: 256\n",
    "        hidden_dropout_prob(float): probability of an element to be zeroed. Default: 0.5\n",
    "        attn_dropout_prob(float): probability of an attention score to be zeroed. Default: 0.5\n",
    "        hidden_act(str): activation function in feed-forward layer. Default: 'gelu'\n",
    "                      candidates: 'gelu', 'relu', 'swish', 'tanh', 'sigmoid'\n",
    "        layer_norm_eps(float): a value added to the denominator for numerical stability. Default: 1e-12\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers=2,\n",
    "        n_heads=2,\n",
    "        hidden_size=64,\n",
    "        inner_size=256,\n",
    "        hidden_dropout_prob=0.5,\n",
    "        attn_dropout_prob=0.5,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-12,\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layer = TransformerLayer(\n",
    "            n_heads,\n",
    "            hidden_size,\n",
    "            inner_size,\n",
    "            hidden_dropout_prob,\n",
    "            attn_dropout_prob,\n",
    "            hidden_act,\n",
    "            layer_norm_eps,\n",
    "        )\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): the input of the TransformerEncoder\n",
    "            attention_mask (torch.Tensor): the attention mask for the input hidden_states\n",
    "            output_all_encoded_layers (Bool): whether output all transformer layers' output\n",
    "\n",
    "        Returns:\n",
    "            all_encoder_layers (list): if output_all_encoded_layers is True, return a list consists of all transformer\n",
    "            layers' output, otherwise return a list only consists of the output of last transformer layer.\n",
    "\n",
    "        \"\"\"\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers\n",
    "\n",
    "\n",
    "class ItemToInterestAggregation(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size, k_interests=5):\n",
    "        super().__init__()\n",
    "        self.k_interests = k_interests  # k latent interests\n",
    "        self.theta = nn.Parameter(torch.randn([hidden_size, k_interests]))\n",
    "\n",
    "    def forward(self, input_tensor):  # [B, L, d] -> [B, k, d]\n",
    "        D_matrix = torch.matmul(input_tensor, self.theta)  # [B, L, k]\n",
    "        D_matrix = nn.Softmax(dim=-2)(D_matrix)\n",
    "        result = torch.einsum(\"nij, nik -> nkj\", input_tensor, D_matrix)  # #[B, k, d]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def construct_transform(config):\n",
    "    if config['unisrec_transform'] is None:\n",
    "        logger = getLogger()\n",
    "        logger.warning('Equal transform')\n",
    "        return Equal(config)\n",
    "    else:\n",
    "        str2transform = {\n",
    "            'plm_emb': PLMEmb\n",
    "        }\n",
    "        return str2transform[config['unisrec_transform']](config)\n",
    "\n",
    "\n",
    "class Equal:\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, dataset, interaction):\n",
    "        return interaction\n",
    "\n",
    "\n",
    "class PLMEmb:\n",
    "    def __init__(self, args):\n",
    "        self.logger = getLogger()\n",
    "        self.logger.info('PLM Embedding Transform in DataLoader.')\n",
    "        self.item_drop_ratio = args.item_drop_ratio\n",
    "\n",
    "    def __call__(self, dataset, interaction):\n",
    "        '''Sequence augmentation and PLM embedding fetching\n",
    "        '''\n",
    "        item_seq_len = interaction['item_length']\n",
    "        item_seq = interaction['item_id_list']\n",
    "\n",
    "        plm_embedding = dataset.plm_embedding\n",
    "        item_emb_seq = plm_embedding(item_seq)\n",
    "        pos_item_id = interaction['item_id']\n",
    "        pos_item_emb = plm_embedding(pos_item_id)\n",
    "\n",
    "        mask_p = torch.full_like(item_seq, 1 - self.item_drop_ratio, dtype=torch.float)\n",
    "        mask = torch.bernoulli(mask_p).to(torch.bool)\n",
    "\n",
    "        # Augmentation\n",
    "        seq_mask = item_seq.eq(0).to(torch.bool)\n",
    "        mask = torch.logical_or(mask, seq_mask)\n",
    "        mask[:,0] = True\n",
    "        drop_index = torch.cumsum(mask, dim=1) - 1\n",
    "\n",
    "        item_seq_aug = torch.zeros_like(item_seq).scatter(dim=-1, index=drop_index, src=item_seq)\n",
    "        item_seq_len_aug = torch.gather(drop_index, 1, (item_seq_len - 1).unsqueeze(1)).squeeze() + 1\n",
    "        item_emb_seq_aug = plm_embedding(item_seq_aug)\n",
    "\n",
    "        interaction.update({\n",
    "            'item_emb_list': item_emb_seq,\n",
    "            'pos_item_emb': pos_item_emb,\n",
    "            'item_id_list_aug': item_seq_aug,\n",
    "            'item_length_aug': item_seq_len_aug,\n",
    "            'item_emb_list_aug': item_emb_seq_aug,\n",
    "        })\n",
    "\n",
    "        return interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'b': 3, 'c': 3}\n"
     ]
    }
   ],
   "source": [
    "x = {'a':1, 'b':2, 'c':3}\n",
    "x.update({'a':2, 'b':3})\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UniSRec(nn.Module):\n",
    "    def __init__(self, args, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_stage = args.train_stage\n",
    "        self.temperature = args.temperature\n",
    "        self.lam = args.lambda_\n",
    "        \n",
    "\n",
    "        assert self.train_stage in [\n",
    "            'pretrain', 'inductive_ft', 'transductive_ft'\n",
    "        ], f'Unknown train stage: [{self.train_stage}]'\n",
    "\n",
    "        if self.train_stage in ['pretrain', 'inductive_ft']:\n",
    "            self.item_embedding = None\n",
    "            # for `transductive_ft`, `item_embedding` is defined in SASRec base model\n",
    "        if self.train_stage in ['inductive_ft', 'transductive_ft']:\n",
    "            # `plm_embedding` in pre-train stage will be carried via dataloader\n",
    "            self.plm_embedding = copy.deepcopy(dataset.plm_embedding)\n",
    "\n",
    "        self.moe_adaptor = MoEAdaptorLayer(\n",
    "            args.n_exps,\n",
    "            args.adaptor_layers,\n",
    "            args.adaptor_dropout_prob\n",
    "        )\n",
    "        self.init_sasrec(args)\n",
    "        \n",
    "    def init_sasrec(self, args):\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.hidden_dropout_prob = args.hidden_dropout_prob\n",
    "        self.layer_norm_eps = args.layer_norm_eps\n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length, self.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=args.n_layers,\n",
    "            n_heads=args.n_heads,\n",
    "            hidden_size=args.hidden_size,\n",
    "            inner_size=args.inner_size,\n",
    "            hidden_dropout_prob=args.hidden_dropout_prob,\n",
    "            attn_dropout_prob=args.attn_dropout_prob,\n",
    "            hidden_act=args.hidden_act,\n",
    "            layer_norm_eps=args.layer_norm_eps,\n",
    "        )\n",
    "        print(self.position_embedding)\n",
    "        \n",
    "    def get_attention_mask(self, item_seq, bidirectional=False):\n",
    "        \"\"\"Generate left-to-right uni-directional or bidirectional attention mask for multi-head attention.\"\"\"\n",
    "        attention_mask = item_seq != 0\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.bool\n",
    "        if not bidirectional:\n",
    "            extended_attention_mask = torch.tril(\n",
    "                extended_attention_mask.expand((-1, -1, item_seq.size(-1), -1))\n",
    "            )\n",
    "        extended_attention_mask = torch.where(extended_attention_mask, 0.0, -10000.0)\n",
    "        return extended_attention_mask\n",
    "    \n",
    "    def gather_indexes(self, output, gather_index):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch\"\"\"\n",
    "        gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1])\n",
    "        output_tensor = output.gather(dim=1, index=gather_index)\n",
    "        return output_tensor.squeeze(1)\n",
    "\n",
    "    def forward(self, item_seq, item_emb, item_seq_len):\n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        print(position_ids)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "\n",
    "        input_emb = item_emb + position_embedding\n",
    "        if self.train_stage == 'transductive_ft':\n",
    "            input_emb = input_emb + self.item_embedding(item_seq)\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb) # B N D\n",
    "\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq)\n",
    "\n",
    "        trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True)\n",
    "        output = trm_output[-1]\n",
    "        output = self.gather_indexes(output, item_seq_len - 1)\n",
    "        return output  # [B H]\n",
    "\n",
    "    def seq_item_contrastive_task(self, seq_output, same_pos_id, interaction):\n",
    "        pos_items_emb = self.moe_adaptor(interaction['pos_item_emb'])\n",
    "        pos_items_emb = F.normalize(pos_items_emb, dim=1)\n",
    "\n",
    "        pos_logits = (seq_output * pos_items_emb).sum(dim=1) / self.temperature\n",
    "        pos_logits = torch.exp(pos_logits)\n",
    "\n",
    "        neg_logits = torch.matmul(seq_output, pos_items_emb.transpose(0, 1)) / self.temperature\n",
    "        neg_logits = torch.where(same_pos_id, torch.tensor([0], dtype=torch.float, device=same_pos_id.device), neg_logits)\n",
    "        neg_logits = torch.exp(neg_logits).sum(dim=1)\n",
    "\n",
    "        loss = -torch.log(pos_logits / neg_logits)\n",
    "        return loss.mean()\n",
    "\n",
    "    def seq_seq_contrastive_task(self, seq_output, same_pos_id, interaction):\n",
    "        item_seq_aug = interaction['item_id_list' + '_aug']\n",
    "        item_seq_len_aug = interaction['item_length' + '_aug']\n",
    "        item_emb_list_aug = self.moe_adaptor(interaction['item_emb_list_aug'])\n",
    "        seq_output_aug = self.forward(item_seq_aug, item_emb_list_aug, item_seq_len_aug)\n",
    "        seq_output_aug = F.normalize(seq_output_aug, dim=1)\n",
    "\n",
    "        pos_logits = (seq_output * seq_output_aug).sum(dim=1) / self.temperature\n",
    "        pos_logits = torch.exp(pos_logits)\n",
    "\n",
    "        neg_logits = torch.matmul(seq_output, seq_output_aug.transpose(0, 1)) / self.temperature\n",
    "        neg_logits = torch.where(same_pos_id, torch.tensor([0], dtype=torch.float, device=same_pos_id.device), neg_logits)\n",
    "        neg_logits = torch.exp(neg_logits).sum(dim=1)\n",
    "\n",
    "        loss = -torch.log(pos_logits / neg_logits)\n",
    "        return loss.mean()\n",
    "\n",
    "    def pretrain(self, interaction):\n",
    "        item_seq = interaction['item_id_list']\n",
    "        item_seq_len = interaction['item_length']\n",
    "        item_emb_list = self.moe_adaptor(interaction['item_emb_list'])\n",
    "        seq_output = self.forward(item_seq, item_emb_list, item_seq_len)\n",
    "        seq_output = F.normalize(seq_output, dim=1)\n",
    "\n",
    "        # Remove sequences with the same next item\n",
    "        pos_id = interaction['item_id']\n",
    "        same_pos_id = (pos_id.unsqueeze(1) == pos_id.unsqueeze(0))\n",
    "        same_pos_id = torch.logical_xor(same_pos_id, torch.eye(pos_id.shape[0], dtype=torch.bool, device=pos_id.device))\n",
    "\n",
    "        loss_seq_item = self.seq_item_contrastive_task(seq_output, same_pos_id, interaction)\n",
    "        loss_seq_seq = self.seq_seq_contrastive_task(seq_output, same_pos_id, interaction)\n",
    "        loss = loss_seq_item + self.lam * loss_seq_seq\n",
    "        return loss\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        if self.train_stage == 'pretrain':\n",
    "            return self.pretrain(interaction)\n",
    "\n",
    "        # Loss for fine-tuning\n",
    "        item_seq = interaction['item_id_list']\n",
    "        item_seq_len = interaction['item_length']\n",
    "        item_emb_list = self.moe_adaptor(self.plm_embedding(item_seq))\n",
    "        seq_output = self.forward(item_seq, item_emb_list, item_seq_len)\n",
    "        test_item_emb = self.moe_adaptor(self.plm_embedding.weight)\n",
    "        if self.train_stage == 'transductive_ft':\n",
    "            test_item_emb = test_item_emb + self.item_embedding.weight\n",
    "\n",
    "        seq_output = F.normalize(seq_output, dim=1)\n",
    "        test_item_emb = F.normalize(test_item_emb, dim=1)\n",
    "\n",
    "        logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1)) / self.temperature\n",
    "        pos_items = interaction[self.POS_ITEM_ID]\n",
    "        loss = self.loss_fct(logits, pos_items)\n",
    "        return loss\n",
    "\n",
    "    def full_sort_predict(self, interaction):\n",
    "        item_seq = interaction['item_id_list']\n",
    "        item_seq_len = interaction['item_length']\n",
    "        item_emb_list = self.moe_adaptor(self.plm_embedding(item_seq))\n",
    "        seq_output = self.forward(item_seq, item_emb_list, item_seq_len)\n",
    "        test_items_emb = self.moe_adaptor(self.plm_embedding.weight)\n",
    "        if self.train_stage == 'transductive_ft':\n",
    "            test_items_emb = test_items_emb + self.item_embedding.weight\n",
    "\n",
    "        seq_output = F.normalize(seq_output, dim=-1)\n",
    "        test_items_emb = F.normalize(test_items_emb, dim=-1)\n",
    "\n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B n_items]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(20, 300)\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    n_layers=2,\n",
    "    n_heads=2,\n",
    "    hidden_size=300,\n",
    "    inner_size = 256,\n",
    "    \n",
    "    hidden_dropout_prob = 0.5,\n",
    "    attn_dropout_prob = 0.5,\n",
    "    hidden_act = 'gelu',\n",
    "    layer_norm_eps = 1e-12,\n",
    "    initializer_range = 0.02,\n",
    "    loss_type = 'CE',\n",
    "    \n",
    "    item_drop_ratio = 0.2,\n",
    "    # item_drop_coefficient = 0.9,\n",
    "    lambda_ = 1e-3,\n",
    "    \n",
    "    train_stage='pretrain',\n",
    "    adaptor_dropout_prob=0.2,\n",
    "    adaptor_layers=[768, 300],\n",
    "    temperature=0.07,\n",
    "    n_exps = 8,\n",
    "    \n",
    "    max_seq_length=20\n",
    ")\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.plm_embedding = torch.nn.Embedding(100, 768)\n",
    "dataset = Dataset()\n",
    "unis_rec = UniSRec(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PLMEmb(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6789, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行一次forward\n",
    "interaction = {}\n",
    "interaction['item_id'] = torch.randint(0, 100, (3,))\n",
    "print(interaction['item_id'].shape)\n",
    "# Add 'item_id' to the dictionary\n",
    "interaction['item_id_list'] = torch.randint(0, 100, (3, 20))\n",
    "interaction['item_length'] = torch.tensor([20, 20, 20])\n",
    "# interaction['item_emb_list'] = torch.randn(3, 20, 768)  # Change the size of the tensor to match 'item_id_list'\n",
    "\n",
    "interaction = transform(dataset, interaction)\n",
    "unis_rec.pretrain(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(20, 64)\n"
     ]
    }
   ],
   "source": [
    "print(unis_rec.position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
